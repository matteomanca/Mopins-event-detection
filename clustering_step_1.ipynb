{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOPINS PROJECT: EVENT DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering step 1:\n",
    "    - input: twitter stream\n",
    "    - output: clusters of tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, I removed the timestamp check when building the clusters. If there are no recent tweets related to a specific topic a new cluster is created automatically \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "from general_functions import *\n",
    "\n",
    "## Define stop words (English and spanish)\n",
    "stop_en = stopwords.words('english')\n",
    "stop_en = stop_en + list(['rt'])\n",
    "stop_sp = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***Create corpus with all documents***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_corpus():\n",
    "    global tf_idf_docs\n",
    "    global dictionary\n",
    "    global tfidf\n",
    "    global documents\n",
    "    \n",
    "    tf_idf_docs = {} ## Reset tf_idf_docs, the docs need to be re-computed based on the new corpus\n",
    "    \n",
    "    texts = [[word for word in document['doc'].lower().split() ]for id_doc,document in documents.items()]\n",
    "    # print texts[1]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # dictionary.save('/tmp/deerwester.dict') # store the dictionary, for future reference\n",
    "    print(len(dictionary))\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***given a query doc q, retrieve a set of 2000 recent documents that have some term co-occurrence with q***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Retrieve a list of most (2000) recent tweets containing at least one term contained in the input tweet \"\"\"\n",
    "def get_docs_with_same_terms(tweet_id):\n",
    "    global documents, td_inv_ind, recent_docs\n",
    "    d2_list = set([])#list of docs containing at least one common term with tweet_id doc\n",
    "    for term in documents[tweet_id]['doc'].split():\n",
    "        if term in td_inv_ind.keys(): \n",
    "            d2_list = set(d2_list).union(set(td_inv_ind[term])) ## set of docs containing the terms of the current tweet\n",
    "            \n",
    "        ### Add term to inverted index\n",
    "        try: ## term already in td_inv_ind \n",
    "            if tweet_id not in td_inv_ind[term]:\n",
    "                td_inv_ind[term].append(tweet_id)\n",
    "        except: ##term NOT in td_inv_ind\n",
    "            td_inv_ind[term] = []\n",
    "            td_inv_ind[term].append(tweet_id)\n",
    "\n",
    "    ###   intersection between d2_list and last 2000 docs\n",
    "    try:\n",
    "        d2_list_ret = set(d2_list).intersection(set(recent_docs))\n",
    "    except Exception as ex:\n",
    "        print ex\n",
    "    return d2_list_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***Given a query doc q, retrieve its nearest neighbour from the list of docs retrieved by `get_docs_with_same_terms`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Look for the doc NN comparing the query doc with those documents retrieved by the get_docs_with_same_terms function \"\"\"\n",
    "def check_nn_inverted_index(d2_list, tweet_id, query):\n",
    "    dis_min = 1\n",
    "    doc_min = -1\n",
    "    global tf_idf_docs, dictionary, tfidf, documents\n",
    "    for d_2 in d2_list:\n",
    "        if d_2 != tweet_id:\n",
    "            try:\n",
    "                d_cand = tf_idf_docs[d_2]\n",
    "            except:\n",
    "                d_cand_tf = dictionary.doc2bow(documents[d_2]['doc'].split()) \n",
    "                d_cand = tfidf[d_cand_tf]\n",
    "                tf_idf_docs[d_2] = d_cand ##add tf_idf for curr doc to avoid to compute it at each loop\n",
    "            c = round(1 - sparse_cos_sim(query, d_cand),4)\n",
    "            if c < dis_min:\n",
    "                dis_min = c\n",
    "                doc_min = d_2\n",
    "    return {'doc':doc_min, 'distance':dis_min}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***Add the query doc q to a cluster***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Given a query_doc, its NN and their distance, create a new cluster with the query_doc or add it to the same cluster of its NN \"\"\"\n",
    "def add_doc_to_clusters(tweet_id, dis_min, doc_min):\n",
    "#     date_format = '%Y-%m-%dT%H:%M:%S'\n",
    "    date_format = '%a %b %d %H:%M:%S +0000 %Y'\n",
    "    \n",
    "    global clusters, inv_cl, usrs_cl, documents_nopreproc, dis_min_t, cl_index\n",
    "#     print 'dis_min ', dis_min\n",
    "    if dis_min >= dis_min_t:     \n",
    "#        print 'Create  new cluster with ', tweet_id\n",
    "        ## Create new cluster\n",
    "        clusters[str(cl_index)] = {}\n",
    "        clusters[str(cl_index)]['docs']=[]\n",
    "        clusters[str(cl_index)]['docs'].append(tweet_id)\n",
    "        clusters[str(cl_index)]['last_timestamp'] = documents_nopreproc[tweet_id]['timestamp']\n",
    "        inv_cl[str(tweet_id)] = cl_index\n",
    "        usrs_cl[str(cl_index)] = []\n",
    "        usrs_cl[str(cl_index)].append(documents_nopreproc[tweet_id]['user'])\n",
    "        clust = cl_index\n",
    "        cl_index += 1\n",
    "    else:       \n",
    "        ## Retrieve the cluster of the most similar doc\n",
    "        clust = inv_cl[str(doc_min)]\n",
    "        \n",
    "        clusters[str(clust)]['docs'].append(tweet_id)\n",
    "        usrs_cl[str(clust)].append(documents_nopreproc[tweet_id]['user'])\n",
    "        inv_cl[str(tweet_id)] = clust ## store in which cluster each doc is \n",
    "    return clust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***Print clusters in real tieme during the algorithm execution***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Print clusters in real tieme during the algorithm execution\"\"\"\n",
    "def print_cluster(clust): \n",
    "    global clusters, thres_min_doc_x_cl, usrs_cl, cluster_file\n",
    "    if len(clusters[str(clust)]['docs']) > thres_min_doc_x_cl: ## I m considering cluster with less than thres_min_doc_x_cl items as neutrals\n",
    "\n",
    "        cluster_file.write('cluster_id:' + str(clust) + '\\n')\n",
    "        cluster_file.write('num_doc:' + str(len(clusters[str(clust)]['docs'])) + '\\n')        \n",
    "        cluster_file.write('num_users:' + str(len(set(usrs_cl[str(clust)])) ) + '\\n')\n",
    "            \n",
    "        if float(float(len(clusters[str(clust)]['docs'])) / len(set(usrs_cl[str(clust)]))) > 2: # Compare #docs wrt #users\n",
    "            cluster_file.write('type: SPAM  \\n')\n",
    "        else:\n",
    "            cluster_file.write('type: EVENT  \\n')\n",
    "            \n",
    "        tmp_print_txt = {}        \n",
    "        cluster_file.write('Docs:  \\n')\n",
    "        \n",
    "        for i in range(0, len(clusters[str(clust)]['docs'])):   \n",
    "                \n",
    "                if remove_all_punct(remove_mentions(remove_rt_str(remove_urls(documents[clusters[str(clust)]['docs'][i]]['doc'])))) not in tmp_print_txt.keys():\n",
    "                    try:                        \n",
    "                        tmp_print_txt[remove_all_punct(remove_mentions(remove_rt_str(remove_urls(documents[clusters[str(clust)]['docs'][i]]['doc']))))] = {'cont':1,'timestamp':documents[clusters[str(clust)]['docs'][i]]['timestamp']}\n",
    "                    except Exception as ex: print \"----------------------------------------------\",ex\n",
    "                else:\n",
    "                    try:\n",
    "                    ## if this doc has already been posted I'm printing the last timestamp\n",
    "                        tmp_print_txt[remove_all_punct(remove_mentions(remove_rt_str(remove_urls(documents[clusters[str(clust)]['docs'][i]]['doc']))))] = {'cont':tmp_print_txt[remove_all_punct(remove_mentions(remove_rt_str(remove_urls(documents[clusters[str(clust)]['docs'][i]]['doc']))))]['cont'] + 1,'timestamp':documents[clusters[str(clust)]['docs'][i]]['timestamp']}\n",
    "                    except Exception as ex: print '=========================================== ', ex\n",
    "        \n",
    "        for text, data in tmp_print_txt.items():    \n",
    "            cluster_file.write('- '+ str(data['timestamp']) + ' --> ' + str(data['cont']) + ' x ' + text + '\\n')\n",
    "        cluster_file.write('\\n=============================== end cluster   ===============================\\n\\n\\n\\n' )\n",
    "        cluster_file.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def code(self, input_point):\n",
    "    \"\"\" Calculate LSH code for a single input point. Returns one code of\n",
    "        length `hash_size` for each `hash_table`.\n",
    "        :param input_point:\n",
    "        A list, or tuple, or numpy ndarray object that contains numbers\n",
    "        only. The dimension needs to be 1 * `input_dim`.\n",
    "        This object will be converted to Python tuple and stored in the\n",
    "        selected storage.\n",
    "        \"\"\"\n",
    "    \n",
    "    if isinstance(input_point, np.ndarray):\n",
    "        input_point = input_point.tolist()\n",
    "    \n",
    "    return [self._hash(self.uniform_planes[i], input_point)\n",
    "            for i in xrange(self.num_hashtables)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***for a given doc q, query the lsh structure and/or the inverted index to find its nearest neighbour***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_clusters(query, tweet_id):\n",
    "\n",
    "    global lsh, inv_doc_index, documents, documents_nopreproc, tfidf, cluster_file, clusters,tf_idf_docs, dis_min_t, dimension,L\n",
    "\n",
    "    query_dense = np.zeros(dimension)\n",
    "    for k,v in dict(query).items():\n",
    "        query_dense[k] = v\n",
    "\n",
    "################ lsh.query rallenta l esecuzione, come limitare il numero di punti in ciascun bucket ??????\n",
    "    try:\n",
    "        near_neigh_data = lsh.query(query_dense, num_results=1, distance_func=\"cosine\") ## query the lsh for the nn\n",
    "    except Exception as ex:\n",
    "        print 'Exception ', ex\n",
    "    \n",
    "    near_neigh_doc = -1\n",
    "    near_neigh_dist = 1\n",
    "    try:\n",
    "        near_neigh_doc = near_neigh_data[0][0][1]\n",
    "        near_neigh_dist = near_neigh_data[0][1]\n",
    "        \n",
    "#        print 'near_neigh_doc ' , near_neigh_doc\n",
    "#        print 'near_neigh_dist ', near_neigh_dist\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    lsh.index(query_dense, tweet_id) ## add new doc to lsh\n",
    "\n",
    "##  limit the number of elements for bucket\n",
    "    hash_keys = code(lsh,query_dense) ## retrieve the list of keys (bucket), 1 key for each hashtable, containing the query\n",
    "    for i in xrange(L):\n",
    "        if len(lsh.hash_tables[i].get_list(hash_keys[i]) ) > doc_x_buck_th:\n",
    "            temp_list = lsh.hash_tables[i].get_list(hash_keys[i])\n",
    "            temp_list.pop(0)\n",
    "            lsh.hash_tables[i].set_val(hash_keys[i],temp_list)\n",
    "\n",
    "            \n",
    "#    \"\"\" if the NN has not been found, check in the inverted index \"\"\"    \n",
    "    if near_neigh_dist >= dis_min_t: # if the NN is not enough similar check in the inverted index of term-document\n",
    "        d2_list = get_docs_with_same_terms(tweet_id) #docs that contain query terms\n",
    "        near_neigh_data_id = check_nn_inverted_index(d2_list, tweet_id, query)\n",
    "        near_neigh_doc = near_neigh_data_id['doc']\n",
    "        near_neigh_dist = near_neigh_data_id['distance']            \n",
    "\n",
    "    clust = add_doc_to_clusters(tweet_id, near_neigh_dist, near_neigh_doc)                \n",
    "\n",
    "#     print 'clust == ', clust\n",
    "#    print_cluster(clust)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ***Save the clustering results and the analyzed documents in json format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Save the clustering results and the analyzed documents in json format\"\"\"\n",
    "\n",
    "def print_final_clusters(all_cluster_file_name,all_users__cluster_file_name):   \n",
    "    global dis_min_t, L, k, dataset\n",
    "    \"\"\"create json containing cluster_id:[list of tweet_ids]\"\"\"\n",
    "    with open(all_cluster_file_name, 'w') as fp:\n",
    "        json.dump(clusters, fp)\n",
    "    \"\"\"create json containing cluster_id:[list of user_ids]    \"\"\"\n",
    "    with open(all_users__cluster_file_name, 'w') as fp:\n",
    "        json.dump(usrs_cl, fp)\n",
    "\n",
    "    \"\"\"  Save documents with processed text  \"\"\"\n",
    "    with open(path + 'documents.json', 'w') as fp:\n",
    "        json.dump(documents, fp)\n",
    "\n",
    "    \"\"\"  Save documents with NOT processed text  \"\"\"\n",
    "    with open(path + 'documents_nopreproc.json', 'w') as fp:\n",
    "        json.dump(documents_nopreproc, fp)\n",
    "\n",
    "    settings = {'L_hashtables':L, 'k_hyperplanes': k, 'threshold_distance': dis_min_t , 'n_docs':len(documents), 'data':dataset}\n",
    "    \"\"\"  Save execution settings  \"\"\"\n",
    "    with open(path + 'settings.json', 'w') as fp:\n",
    "        json.dump(settings, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_print(path, all_cluster_file_name, all_users_cluster_file_name):\n",
    "    clusters_file = open(path + 'final_print_clusters.txt','a') ##output file\n",
    "    with open(all_cluster_file_name) as data_file:    \n",
    "        cluster_res = json.load(data_file)\n",
    "\n",
    "    with open(all_users_cluster_file_name) as users_data_file:    \n",
    "        usrs_cl = json.load(users_data_file)\n",
    "\n",
    "    with open(path + \"documents.json\") as doc_data_file:    \n",
    "        documents = json.load(doc_data_file)\n",
    "\n",
    "    with open(path + \"documents_nopreproc.json\") as documents_nopreproc_data_file:    \n",
    "        documents_nopreproc = json.load(documents_nopreproc_data_file)\n",
    "            \n",
    "    min_ndocs = 0 ## treshold\n",
    "    for k,v in cluster_res.items():\n",
    "#        tmp_print_txt = {}\n",
    "        tmp_print_txt = OrderedDict()\n",
    "\n",
    "        distinct_users = len(set(usrs_cl[k])) #number of distinct users in cluster k\n",
    "        tot_docs = len(v['docs']) #number of total docs in cluster k\n",
    "        if tot_docs > min_ndocs:  ## decide what we want to print...not relevant now, we can read the result  of the clustering and print whatever we want\n",
    "            for i in range(len(v['docs'])):\n",
    "                text_without_nline = ''.join(ch for ch in remove_all_punct(remove_mentions(remove_rt_str(remove_urls(documents[str(v['docs'][i])]['doc'])))) if ch not in ['\\n','\\r'])\n",
    "                if text_without_nline not in tmp_print_txt.keys():\n",
    "                    tmp_print_txt[text_without_nline] =  {'cont':1,'timestamp':documents[str(v['docs'][i])]['timestamp'], 'tweet_id':v['docs'][i]}\n",
    "                else:\n",
    "                    try:\n",
    "                        tmp_print_txt[text_without_nline] =  {'cont':tmp_print_txt[text_without_nline]['cont']+1,'timestamp':documents[str(v['docs'][i])]['timestamp'], 'tweet_id':v['docs'][i]}\n",
    "                    except Exception as ex:\n",
    "                        print 'qqq ' ,  ex\n",
    "            distinct_docs = len(tmp_print_txt)\n",
    "            if(len(tmp_print_txt) > 0):\n",
    "                clusters_file.write('\\n')\n",
    "                clusters_file.write('\\n')\n",
    "                clusters_file.write('CLUSTER ' + str(k) + '||  DISTINCT USERS '+ str( len(set(usrs_cl[k])) ) + '|| TOT DOCS '+ str(len(set(cluster_res[k]['docs'])) ) + '|| DISTINCT DOCS ' + str(len(tmp_print_txt) ) + '\\n')\n",
    "                clusters_file.write('Timestamp | freq | Text \\n')\n",
    "\n",
    "                for text, data in tmp_print_txt.items():    \n",
    "                    clusters_file.write('- ' + str(data['timestamp']) + ' | ' + str(data['cont']) + ' | ' + remove_rt_str(remove_mentions(documents_nopreproc[str(data['tweet_id'])]['doc'])).encode('ascii','ignore') + '\\n')\n",
    "    clusters_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_collection(data):\n",
    "    global dictionary, tfidf, documents, n_docs, tweets_db_file, recent_docs, L, lsh, dimension\n",
    "    try:\n",
    "        if (1): #data.lang=='en'\n",
    "\n",
    "            #get tweet info\n",
    "            tweet_text = data['text']\n",
    "            tweet_id = data['id']\n",
    "            \n",
    "            #Avoid duplicates\n",
    "            if tweet_id in documents.keys():\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                tweet_timestamp = data['created_at'] ## Correct way\n",
    "            except:\n",
    "                tweet_timestamp = datetime.fromtimestamp(int(str(data['createdAt'])[0:len( str(data['createdAt']) )-3])).strftime('%a %b %d %H:%M:%S +0000 %Y') ## for catalunya BDigital dataset\n",
    "\n",
    "#            print tweet_timestamp\n",
    "            user_id = data['user']['id']\n",
    "            \n",
    "            tweet_text = stem_doc(remove_stop_words(remove_punct(remove_mentions(remove_rt_str(remove_urls(tweet_text))))) )\n",
    "\n",
    "#            print 'n_docs = ', n_docs\n",
    "            n_docs += 1\n",
    "            documents[tweet_id] = {}\n",
    "            documents_nopreproc[tweet_id] = {}\n",
    "            documents[tweet_id] = {'doc':tweet_text, 'timestamp':tweet_timestamp,'user':user_id}\n",
    "            documents_nopreproc[tweet_id] = {'doc':data['text'], 'timestamp':tweet_timestamp,'user':user_id}\n",
    "            \n",
    "            recent_docs.append(tweet_id)\n",
    "            \n",
    "            if n_docs % training_thres == 0:\n",
    "                \n",
    "                #######\n",
    "                \"\"\"  Print clusters. Set the output file name based on your input dataset  \"\"\"\n",
    "                all_cluster_file_name = path + \"clusters_step1.json\"\n",
    "                all_users__cluster_file_name = path + \"users_clusters_step1.json\"\n",
    "                print_final_clusters(all_cluster_file_name,all_users__cluster_file_name)\n",
    "\n",
    "                custom_print(path, all_cluster_file_name,all_users__cluster_file_name)\n",
    "                \n",
    "                os.system(\"python cl_step_2.py\")\n",
    "                os.system(\"python summerise.py\")\n",
    "\n",
    "                #######\n",
    "\n",
    "\n",
    "                update_corpus()\n",
    "                dimension = len(dictionary)\n",
    "                lsh = LSHash(k, dimension, L)   \n",
    "\n",
    "\n",
    "            if n_docs > training_thres:\n",
    "                query_vec = dictionary.doc2bow(tweet_text.split())\n",
    "                query = tfidf[query_vec]\n",
    "                tf_idf_docs[tweet_id] = query\n",
    "                if len(query_vec) > 0:\n",
    "                    if float(float(len(tweet_text.split()) ) / len(query_vec)) <= 2:\n",
    "                        build_clusters(query,tweet_id)\n",
    "                return True\n",
    "    except Exception as ex: \n",
    "        print ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':  \n",
    "    cl_index = 0\n",
    "    documents_nopreproc = OrderedDict()\n",
    "    documents = OrderedDict()\n",
    "\n",
    "    recent_docs_threshold = 2000 ## number of docs to consider for the inverted index\n",
    "    recent_docs = collections.deque(maxlen=recent_docs_threshold) #contains the 'recent_docs_threshold' most recent tweet_ids\n",
    "\n",
    "    ##----- Parameters\n",
    "\n",
    "    # For every training_thres docs arrived in the streaming The corpus is updated\n",
    "    training_thres = 3000\n",
    "\n",
    "    dis_min_t = 0.8 #minimum distance to add doc to NN cluster\n",
    "    k = 13 #number of hyperplanes k \n",
    "    L = 16 #math.log(0.025,0.8)\n",
    "\n",
    "    thres_min_doc_x_cl = 3   ## useful just during the real time printing\n",
    "    doc_x_buck_th = 100 ## limit of the #docs in a single bucket ... SHOULD WE CHANGE THIS NUMBER ???\n",
    "\n",
    "    ## Structures    \n",
    "    inv_doc_index = [] ## For each tweet contains the corresponding hash_key\n",
    "\n",
    "#    clusters = {} #Contain the result of the clustering. cluster_id:[list of tweets]\n",
    "    clusters = OrderedDict()\n",
    "    usrs_cl = {} #Contain the user of the result of the clustering. cluster_id:[list of users]\n",
    "    inv_cl = {} # for each tweet it contains the cluster_id\n",
    "\n",
    "    td_inv_ind = {} ##Inverted index structure that contains for each term the list of tweets (only tweets_id is stored) that contain that term\n",
    "    n_docs = 0 ## doc counter\n",
    "    tf_idf_docs = {} # Store the tf-idf of each arrived tweet (to avoid to compute it multiple times)\n",
    "\n",
    "\n",
    "    \"\"\"  Set the output file name based on your input dataset  \"\"\"    \n",
    "    cluster_file_name = 'real_time_clusters.txt'\n",
    "    cluster_file = open(cluster_file_name,'w') ## this file contains the cluster created in real time during the algorithm execution\n",
    "    \n",
    "    ## Initialize corpus\n",
    "    texts = [[word for word in document['doc'].lower().split() if (word not in stop_en) and (word not in stop_sp)]for id_doc,document in documents.items()]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    \n",
    "    \n",
    "    dimension = len(dictionary)\n",
    "    lsh = LSHash(k, dimension, L)\n",
    "\n",
    "    \n",
    "    \"\"\"  SET THE PATH OF YOUR INPUT DATASET   \"\"\"\n",
    "#    dataset = 'twitter-crisis-test'\n",
    "    path = 'test-disaster/'\n",
    "    indir = 'data'\n",
    "    for f in os.listdir(indir+'/'):\n",
    "        try:            \n",
    "            user_file = open(indir +'/'+ f)\n",
    "            for l in user_file:\n",
    "                try: \n",
    "                    tweet_js = json.loads(l.strip())\n",
    "                    create_collection(tweet_js)\n",
    "                except Exception as ex:\n",
    "                    print 'xxx ',  ex\n",
    "\n",
    "            user_file.close()\n",
    "            print len(documents)\n",
    "        except:pass\n",
    "    cluster_file.close()        \n",
    "    \n",
    "\n",
    "    \"\"\"  Print clusters. Set the output file name based on your input dataset  \"\"\"    \n",
    "    all_cluster_file_name = path + \"clusters_step1.json\"\n",
    "    all_users__cluster_file_name = path + \"users_clusters_step1.json\"\n",
    "    print_final_clusters(all_cluster_file_name,all_users__cluster_file_name)\n",
    "    print '\\n number of documents = ', len(documents)\n",
    "    print 'L = ', L\n",
    "    print 'min distance = ', dis_min_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
