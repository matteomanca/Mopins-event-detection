{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOPINS PROJECT: EVENT DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise:\n",
    "    - input: Clusters/Events obatined in clustering step 2\n",
    "    - output: Summary for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from general_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"PARAMETERS\"\"\"\n",
    "path = 'test-disaster/' ## Set your path\n",
    "cluster_res_file = 'cluster_res2_step2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read clustering result\"\"\"\n",
    "with open(path + cluster_res_file) as data_file:    \n",
    "    cluster_res = json.load(data_file)\n",
    "    \n",
    "\"\"\"Read documents\"\"\"\n",
    "with open(path + \"documents.json\") as doc_data_file:    \n",
    "    documents = json.load(doc_data_file)\n",
    "\n",
    "with open(path + \"documents_nopreproc.json\") as documents_nopreproc_data_file:    \n",
    "    documents_nopreproc = json.load(documents_nopreproc_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the size of each cluster and filter clusters containing at least `th_event` documents (In our case we consider all clusters, so $th\\_event=0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = {}\n",
    "for k, v in cluster_res.items():\n",
    "    dims[k] = len(v)\n",
    "\n",
    "## Select clusters with more than th_event docs\n",
    "th_event = 0\n",
    "active_clusters = dict((k, v) for k, v in dims.items() if v >= th_event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Compute centroid for a set of documents\"\"\"\n",
    "def get_centroid(v):\n",
    "\n",
    "    global tf_idf_docs\n",
    "    global dictionary\n",
    "    global tfidf\n",
    "\n",
    "    cont = 0\n",
    "    curr_cent = {}\n",
    "    for i in v: #list of docs in current cluster\n",
    "        try:\n",
    "            curr_tfidf_d1 = tf_idf_docs[str(i)]\n",
    "        except:\n",
    "            tf_d1 =  dictionary.doc2bow(documents[str(i)]['doc'].split())\n",
    "            curr_tfidf_d1 = tfidf[tf_d1]      \n",
    "            tf_idf_docs[str(i)] = curr_tfidf_d1 #save to avoid multiple computation of tfidf of the same tweet\n",
    "\n",
    "        curr_doc = dict(curr_tfidf_d1)\n",
    "\n",
    "        for key in set(curr_doc.keys()):    \n",
    "#             print key\n",
    "            try:\n",
    "                curr_cent[key] = curr_cent[key] + curr_doc[key]\n",
    "            except:\n",
    "                curr_cent[key] = curr_doc[key]\n",
    "    curr_centroid = [(term_id,(tfidfsum/len(v)) ) for term_id,tfidfsum in curr_cent.items()]\n",
    "    return curr_centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for the centroide nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn(cluster_centroid, docs):\n",
    "    dist_th = 2\n",
    "    nn_doc = \"\"\n",
    "    global tf_idf_docs    \n",
    "    for d in docs:\n",
    "        tf_idf_d = tf_idf_docs[str(d)]\n",
    "        dist_doc_centroid = round(1 - sparse_cos_sim(cluster_centroid, tf_idf_d),4)\n",
    "        if dist_doc_centroid < dist_th:\n",
    "            dist_th = dist_doc_centroid\n",
    "            nn_doc = d\n",
    "    return nn_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look for the most different doc from the set of docs to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_div_doc(cand_set, docs):\n",
    "    cs_centroid = get_centroid(cand_set)\n",
    "    dist_th = 0\n",
    "    div_doc = \"\"\n",
    "    global tf_idf_docs    \n",
    "    for d in docs:\n",
    "        tf_idf_d = tf_idf_docs[str(d)]\n",
    "        dist_doc_cs_centroid = round(1 - sparse_cos_sim(cs_centroid, tf_idf_d),4)\n",
    "        if dist_doc_cs_centroid > dist_th:\n",
    "            dist_th = dist_doc_cs_centroid\n",
    "            div_doc = d\n",
    "    return {'doc':div_doc,'dist':dist_th}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summ_sets = OrderedDict()\n",
    "tf_idf_docs = {} ## Reset tf_idf_docs, the docs need to be re-computed based on the new corpus    \n",
    "texts = [[word for word in document['doc'].lower().split() ]for id_doc,document in documents.items()]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "for k, v in active_clusters.items():\n",
    "    docs = cluster_res[k]\n",
    "    cluster_centroid = get_centroid(docs)    \n",
    "    summ_sets[k] = []\n",
    "    nn_doc = get_nn(cluster_centroid, docs)\n",
    "    docs.remove(nn_doc) ## this remove also from cluster_res[k]['docs']\n",
    "    summ_sets[k].append(nn_doc)\n",
    "    add_docs = True\n",
    "    while add_docs:\n",
    "        div_doc = get_div_doc(summ_sets[k],docs)\n",
    "        ## if the document is enough \"diverse\" I add to the list of docs to retrieve to the user\n",
    "        if div_doc['dist'] > 0.8 :\n",
    "            summ_sets[k].append(div_doc['doc'])\n",
    "            docs.remove(div_doc['doc']) ## this remove also from cluster_res[k]['docs']\n",
    "        else:\n",
    "            add_docs = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write summaries to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path + \"cl_tags.json\") as data_file:\n",
    "    cl_tags_dict = json.load(data_file)\n",
    "\n",
    "html = '<html><head></head><body>'\n",
    "\n",
    "summary_name = 'summary.txt'\n",
    "summary_file = open(path + summary_name,'a') ## this file contains the cluster created in real time during the algorithm execution\n",
    "\n",
    "summary_file_html = open(path + 'summary.html','a') ## this file contains the cluster created in real time during the algorithm execution\n",
    "\n",
    "summary_file.write('===============================================================================\\n')\n",
    "summary_file.write(str(len(documents)) + ' documents\\n')\n",
    "summary_file.write('=============================================\\n\\n')\n",
    "\n",
    "html = html + '<br/><br/>' + str(len(documents)) + ' documents\\n' + '<br />'\n",
    "html = html + '<table style=\"width:80%\" >'\n",
    "\n",
    "print '\\n\\n'\n",
    "for k, docs in summ_sets.items():\n",
    "#    summary_file.write('Cluster ' + str(k) +  ' - ' + 'original ' +  str(active_clusters[k]) +  ' - '  +  'summary ' +  str(len(docs)) +  '\\n')\n",
    "    summary_file.write('Summary of Event ' + str(k)  +  ' (Total tweets: ' + str(len(cluster_res[k])) + ') \\n')\n",
    "    html = html + '<th bgcolor=\"#FAD7A0\" colspan=2>Event ' + str(k) + ' - (Total tweets: ' + str(len(cluster_res[k])) + ')</th>'\n",
    "    try:\n",
    "        summary_file.write(cl_tags_dict[k] +  '\\n')\n",
    "        html = html + '<tr bgcolor=\"#FAD7A0\"><td colspan=2>' + cl_tags_dict[k]  + '</td></tr>'\n",
    "    except:\n",
    "        print 'cccc'\n",
    "\n",
    "    summary_sorted = OrderedDict()\n",
    "    for d in docs:\n",
    "        summary_sorted[documents_nopreproc[str(d)]['timestamp']] = remove_mentions(remove_rt_str(documents_nopreproc[str(d)]['doc']))\n",
    "    summary_sorted = collections.OrderedDict(sorted(summary_sorted.items()))\n",
    "\n",
    "    i = 0\n",
    "    for time, body in summary_sorted.items():\n",
    "        i += 1\n",
    "        summary_file.write('- ' + str(time) + ' ' + str(body)+ '\\n')\n",
    "        if i%2 == 0:\n",
    "            html = html + '<tr bgcolor=\"#D6EAF8\"><td width = 10%>' +str(time)  + '</td><td>' + str(body)  + '</td></tr>'\n",
    "        else:\n",
    "            html = html + '<tr bgcolor=\"#D1F2EB\"><td width = 10%>' +str(time)  + '</td><td>' + str(body)  + '</td></tr>'\n",
    "    html = html + '<br/><br/>'\n",
    "    summary_file.write(\"\\n\\n\")\n",
    "summary_file.write('===============================================================================\\n\\n\\n')\n",
    "\n",
    "html = html + '</table></body></html>'\n",
    "summary_file_html.write(html)\n",
    "\n",
    "summary_file_html.close()\n",
    "summary_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
