{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOPINS PROJECT: EVENT DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering step 2:\n",
    "    - input: Clusters obatined in clustering step 1\n",
    "    - output: Events, i.e., clusters of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, I removed the timestamp check when building the clusters. If there are no recent tweets related to a specific topic a new cluster is created automatically \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "from general_functions import *\n",
    "\n",
    "\"\"\"PARAMETERS\"\"\"\n",
    "path = 'test-disaster/' ## Set your path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags(text):\n",
    "    return[i for i in text.lower().split() if '#' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Given a query_doc, its NN and their distance, create a new cluster with the query_doc or add it to the same cluster of its NN \"\"\"\n",
    "def add_doc_to_clusters(clust_id, dis_min, doc_min):\n",
    "    global clusters, inv_cl, usrs_cl, documents_nopreproc, dis_min_t, cl_index, original_clusters\n",
    "\n",
    "    if dis_min > dis_min_t:\n",
    "#        print 'Create  new cluster with ', clust_id, '\\n\\n  '\n",
    "        ## Create new cluster\n",
    "        cl_index = min(original_clusters[clust_id]['docs'])\n",
    "        clusters[str(cl_index)] = []\n",
    "        clusters[str(cl_index)].append(clust_id)\n",
    "        inv_cl[str(clust_id)] = cl_index\n",
    "        clust = cl_index\n",
    "    else:\n",
    "        ## Retrieve the cluster of the most similar doc\n",
    "        clust = inv_cl[str(doc_min)]\n",
    "        ## Add query doc to the same cluster of the most similar doc\n",
    "        clusters[str(clust)].append(clust_id)\n",
    "        inv_cl[str(clust_id)] = clust ## store in which cluster each doc is \n",
    "    return clust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create bag-of-words for each cluster and compute tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read clusters step 1\"\"\"\n",
    "all_clusters_file_name = path + \"clusters_step1.json\" #file containing all obtained clusters cluster_id:[clust_ids]\n",
    "# all_users_clusters_file_name = \"users_clusters.cat0315.end.json\" #file containing all obtained clusters cluster_id:[clust_ids]\n",
    "\n",
    "users_clusters_file_name = path + \"users_clusters_step1.json\" #file containing all obtained clusters cluster_id:[clust_ids]\n",
    "\n",
    "with open(all_clusters_file_name) as data_file:    \n",
    "    original_clusters = json.load(data_file)\n",
    "\n",
    "with open(users_clusters_file_name) as data_file:    \n",
    "    original_clusters_users = json.load(data_file)\n",
    "\n",
    "with open(path + \"documents.json\") as doc_data_file:    \n",
    "    documents = json.load(doc_data_file)\n",
    "\n",
    "with open(path + \"documents_nopreproc.json\") as documents_nopreproc_data_file:    \n",
    "    documents_nopreproc = json.load(documents_nopreproc_data_file)\n",
    "\n",
    "#num_doc_x_clust_th = 2        ### CHECK\n",
    "#num_users_x_clust_th = 2      ### CHECK\n",
    "#\n",
    "num_doc_x_clust_th = len(documents) * 0.1 /100 # 0.2%       ### CHECK: for 3000 tweets I consider a cluster only if ti contains at least 10 docs\n",
    "#num_doc_x_clust_th = len(documents) * 0.5 /100 # 0.2%       ### CHECK: for 3000 tweets I consider a cluster only if ti contains at least 10 docs\n",
    "num_users_x_clust_th = num_doc_x_clust_th /20      ### CHECK\n",
    "\n",
    "\"\"\"Create bag of words for each cluster\"\"\"\n",
    "bags = OrderedDict() #dictionary cluster_id: bag of words\n",
    "#print original_clusters.keys()\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# create bag of words for each cluser\n",
    "###\n",
    "for k, v in collections.OrderedDict(sorted(original_clusters.items())).iteritems():\n",
    "    \"\"\"TODO : PAY ATTENTION TO THE FOLLOWING CONDITION: I' M NOT CONSIDERING ALL CLUSTERS BUT JUST THOSE BIGGER THAN num_doc_x_clust_th DOCS\"\"\"\n",
    "\n",
    "## remove neutral and spam cluster\n",
    "    if len(v['docs']) > num_doc_x_clust_th and len(original_clusters_users[k]) > num_users_x_clust_th : \n",
    "        for i in range(len(v['docs'])):\n",
    "            try:\n",
    "                bags[k] = bags[k] + \" \" + documents[str(v['docs'][i])]['doc']  #texts are already stopped and stemmed\n",
    "            except:\n",
    "                bags[k] = documents[str(v['docs'][i])]['doc']\n",
    "\n",
    "# save bags of words\n",
    "with open(path + \"bags.json\", 'w') as fp:\n",
    "    json.dump(bags, fp)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "#create corpus with all terms of my clusters\"\"\"\n",
    "###\n",
    "tf_idf_clusters = OrderedDict()\n",
    "\n",
    "texts_bag = [[word for word in text.lower().split()] for id_clus, text in bags.items()]\n",
    "# print texts_bag[1]\n",
    "\n",
    "dictionary_bag = corpora.Dictionary(texts_bag)\n",
    "# dictionary.save('/tmp/deerwester.dict') # store the dictionary, for future reference\n",
    "# print(len(dictionary_bag))\n",
    "\n",
    "corpus_bag = [dictionary_bag.doc2bow(text) for text in texts_bag]\n",
    "\n",
    "tfidf_bag = models.TfidfModel(corpus_bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Each cluster is represented by a tf-idf vector \"\"\" \n",
    "\n",
    "for k, v in bags.items():\n",
    "    curr_tf = dictionary_bag.doc2bow(v.split()) \n",
    "    curr_tfidf = tfidf_bag[curr_tf]                    \n",
    "    tf_idf_clusters[k] = curr_tfidf ##add tf_idf for curr doc to avoid to compute it at each loop\n",
    "\n",
    "k = 13 #number of hyperplanes k, put 13\n",
    "L = 16 #math.log(0.025,0.8) ==> 16\n",
    "\n",
    "dimension = len(dictionary_bag)    \n",
    "\n",
    "doc_x_buck_th = 2000\n",
    "\n",
    "## Structures\n",
    "lsh = LSHash(k, dimension, L)\n",
    "inv_doc_index = [] ## For each tweet contains the corresponding hash_key\n",
    "\n",
    "clusters = OrderedDict() #Contain the result of the clustering. cluster_id:[list of tweets]\n",
    "\n",
    "usrs_cl = {} #Contain the user of the result of the clustering. cluster_id:[list of users]\n",
    "inv_cl = {} # for each tweet it contains the cluster_id\n",
    "\n",
    "\n",
    "dis_min_t = 0.85 #a high value leads to more spread clusters\n",
    "\n",
    "cl_index = 0\n",
    "\n",
    "\n",
    "\n",
    "for cl_id, tfidf_values in tf_idf_clusters.items():\n",
    "    ## Initialize distance and NN doc\n",
    "    near_neigh_doc = -1\n",
    "    near_neigh_dist = 1\n",
    "\n",
    "    query_dense = np.zeros(dimension)\n",
    "    for k,v in dict(tfidf_values).items():\n",
    "        query_dense[k] = v\n",
    "    try:\n",
    "        near_neigh_data = lsh.query(query_dense, num_results=1, distance_func=\"cosine\") ## query the lsh for the nn\n",
    "    except Exception as ex:\n",
    "        print 'Exception ', ex\n",
    "\n",
    "    try:\n",
    "        near_neigh_doc = near_neigh_data[0][0][1]\n",
    "        near_neigh_dist = near_neigh_data[0][1]\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    lsh.index(query_dense, cl_id) ## add new doc to lsh\n",
    "    \n",
    "    add_doc_to_clusters(cl_id, near_neigh_dist, near_neigh_doc)                \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\"\"\"Print clusters of clusters\"\"\"\n",
    "cluster_res = OrderedDict()\n",
    "\n",
    "\"\"\"save cluster result step 2\"\"\"\n",
    "with open(path + \"cluster_res_XXX.json\", 'w') as fp:\n",
    "    json.dump(clusters, fp)\n",
    "\n",
    "\n",
    "for cl_id, cl_list in clusters.items(): #each cluster of clusters\n",
    "    if len(cl_list) > 1:\n",
    "        docs = []\n",
    "        for cluster in sorted(cl_list): #get documents for each cluster_id\n",
    "            try:\n",
    "                if {cluster: original_clusters[cluster]} not in cluster_res[cl_id]:\n",
    "                    cluster_res[cl_id].append({cluster: original_clusters[cluster]})\n",
    "            except:\n",
    "                cluster_res[cl_id] = []\n",
    "                if {cluster: original_clusters[cluster]} not in cluster_res[cl_id]:\n",
    "                    cluster_res[cl_id].append({cluster: original_clusters[cluster]})\n",
    "\n",
    "\"\"\"save cluster result step 2\"\"\"\n",
    "with open(path + \"cluster_res_step2.json\", 'w') as fp:\n",
    "    json.dump(cluster_res, fp)\n",
    "\n",
    "\n",
    "\n",
    "cluster_res2 = OrderedDict()\n",
    "for cl_id2, cl_data2 in cluster_res.items():\n",
    "    for cls_1 in sorted(cl_data2):\n",
    "        for cl_id1, cl_data1 in cls_1.items():\n",
    "            try:\n",
    "                cluster_res2[str(cl_id2)] += cl_data1['docs']\n",
    "            except:\n",
    "                cluster_res2[str(cl_id2)] = cl_data1['docs']\n",
    "\n",
    "\"\"\"save cluster result step 2 in diferent format: cl_id: list of all docs (that belong to diferent clusters in step 1)\"\"\"\n",
    "with open(path + \"cluster_res2_step2.json\", 'w') as fp:\n",
    "    json.dump(cluster_res2, fp)\n",
    "    \n",
    "\n",
    "with open(path + \"cluster_res_step2.json\") as data_file:    \n",
    "    cluster_res = json.load(data_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "#Print events\n",
    "##\n",
    "\n",
    "cluster_file_name = 'cluster_step2XXX.txt'\n",
    "cluster_file = open(path + cluster_file_name,'a') ## this file contains the cluster created in real time during the algorithm\n",
    "\n",
    "cluster_file.write('===============================================================================\\n')\n",
    "cluster_file.write(str(len(documents)) + ' documents\\n')\n",
    "cluster_file.write('=============================================\\n\\n')\n",
    "\n",
    "cl_tags_dict = OrderedDict()\n",
    "\n",
    "\n",
    "\n",
    "for cl_id, cl_data in cluster_res.items():\n",
    "    cl_tags = []\n",
    "    tweets_concat_str = ''\n",
    "    temp_list = []\n",
    "    to_print = {}\n",
    "    cluster_file.write('Event: ' + cl_id + '\\n')\n",
    "    cluster_file.write('Cluter_dim: ' + str(len(cluster_res2[cl_id]) ) + '\\n')\n",
    "    for cluster_1 in cl_data:\n",
    "        for cl_step1_id , cl_step1_data in cluster_1.items():\n",
    "            temp_list.append(cl_step1_id)\n",
    "            current_cluster_doc_list = cl_step1_data['docs']\n",
    "            for d in current_cluster_doc_list:\n",
    "                text_current_doc = remove_all_punct(remove_mentions(remove_rt_str(remove_urls(documents_nopreproc[str(d)]['doc']))))\n",
    "                time_current_doc = documents_nopreproc[str(d)]['timestamp']\n",
    "\n",
    "                if text_current_doc not in tweets_concat_str:\n",
    "                    tweets_concat_str = tweets_concat_str + ' ' + text_current_doc\n",
    "                    to_print[time_current_doc] = {'text':text_current_doc, 'id': cl_id}\n",
    "\n",
    "                curr_tags = get_tags(documents_nopreproc[str(d)]['doc'])\n",
    "#                print 'curr_tags',  curr_tags\n",
    "                if len(curr_tags) > 0 :\n",
    "                    cl_tags.append(get_tags(documents_nopreproc[str(d)]['doc']) )\n",
    "            cl_tags_dict[cl_id] = ' '.join(set(list(itertools.chain(*cl_tags))) )\n",
    "\n",
    "    cluster_file.write(cl_tags_dict[cl_id] + '\\n')\n",
    "    ## Sort by timestamp\n",
    "    to_print = collections.OrderedDict(sorted(to_print.items()))\n",
    "    for time, data in to_print.items():\n",
    "        cluster_file.write(' - ' +  time + ' ' + data['text'] + '\\n')\n",
    "    cluster_file.write('Cluters step 1: ' + ' '.join(sorted(temp_list)) + '\\n')\n",
    "    cluster_file.write('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
